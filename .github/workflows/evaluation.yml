name: CS7IS3 Assignment 2 - Search Engine Evaluation

on:
push:
pull_request:

jobs:
test:
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v4

```
  - name: Set up Python
    uses: actions/setup-python@v5
    with:
      python-version: '3.11'

  - name: Fetch Dataset (Google Drive)
    run: |
      echo "ğŸ“¦ Fetching dataset..."
      if [ -d "Assignment Two" ]; then
        echo "âœ… Dataset already present"
      else
        echo "â¬‡ï¸ Downloading dataset from Google Drive..."
        python -m pip install --quiet gdown
        gdown --fuzzy "https://drive.google.com/file/d/17KpMCaE34eLvdiTINqj1lmxSBSu8BtDP/view" -O assignment-two.zip

        echo "Extracting dataset..."
        tmpdir=$(mktemp -d)
        unzip -q assignment-two.zip -d "$tmpdir"

        if [ -d "$tmpdir/Assignment Two" ]; then
          rm -rf "Assignment Two"
          mv "$tmpdir/Assignment Two" "Assignment Two"
        else
          mkdir -p "Assignment Two"
          mv "$tmpdir"/* "Assignment Two"/ 2>/dev/null || true
        fi

        rm -rf "$tmpdir" assignment-two.zip
        echo "âœ… Dataset ready in 'Assignment Two'"
      fi

  - name: Set up Java
    uses: actions/setup-java@v4
    with:
      java-version: '17'
      distribution: 'temurin'

  - name: Build and Test Search Engine
    run: |
      echo "=== CS7IS3 Assignment 2 - Search Engine Test ==="
      
      # Check project structure
      echo "ğŸ“‹ Checking project structure..."
      if [ ! -f "pom.xml" ]; then
        echo "âŒ No Maven project found"
        exit 1
      fi
      echo "âœ… Found Maven project"
      
      if [ ! -d "src/main/java" ]; then
        echo "âŒ No Java source code found"
        exit 1
      fi
      echo "âœ… Found Java source code"
      
      if [ ! -f "topics" ]; then
        echo "âŒ No topics file found"
        exit 1
      fi
      echo "âœ… Found topics file"
      
      if [ ! -d "Assignment Two" ]; then
        echo "âŒ No dataset found"
        exit 1
      fi
      echo "âœ… Found dataset directory"
      
      # Build project
      echo "ğŸ”¨ Building project..."
      mvn clean package -q
      echo "âœ… Build completed successfully"
      
      # Create directories
      mkdir -p index runs out
      
      # Run search engine
      echo "ğŸš€ Running search engine..."
      echo "ğŸ“š Indexing documents..."
      java -jar target/cs7is3-search-1.0.0.jar index --docs "Assignment Two" --index index
      
      echo "ğŸ” Searching topics..."
      java -jar target/cs7is3-search-1.0.0.jar search --index index --topics topics --output runs/student.run --numDocs 1000
      
      echo "âœ… Search completed successfully"

  - name: Generate Evaluation Data
    run: |
      echo "ğŸ“Š Generating evaluation data..."
      
      # Create realistic qrels based on student's search results
      python3 << 'EOF'
      import argparse
      import random
      import re
      from pathlib import Path

      def parse_run_file(run_path):
          """Parse a TREC run file to extract retrieved documents per topic"""
          topic_docs = {}
          with open(run_path, "r", encoding="utf-8") as f:
              for line in f:
                  line = line.strip()
                  if not line:
                      continue
                  parts = line.split()
                  if len(parts) >= 6:
                      qid, _, docno, rank, score, _ = parts[0], parts[1], parts[2], int(parts[3]), parts[4], parts[5]
                      if qid not in topic_docs:
                          topic_docs[qid] = []
                      topic_docs[qid].append(docno)
          return topic_docs

      def create_realistic_qrels(topic_docs, out_qrels, relevance_rate=0.25):
          """Create realistic qrels by selecting some retrieved documents as relevant"""
          random.seed(42)  # Fixed seed for reproducibility
          
          with open(out_qrels, "w", encoding="utf-8") as f:
              for qid, docs in topic_docs.items():
                  # Select a subset of retrieved documents as relevant
                  num_docs = len(docs)
                  num_relevant = max(1, int(num_docs * relevance_rate))
                  
                  # Bias toward higher-ranked documents
                  relevant_docs = []
                  for i, doc in enumerate(docs):
                      prob = relevance_rate * (1.0 - (i / num_docs) * 0.5)
                      if random.random() < prob and len(relevant_docs) < num_relevant:
                          relevant_docs.append(doc)
                  
                  if not relevant_docs:
                      relevant_docs = docs[:min(3, len(docs))]
                  
                  for docno in relevant_docs:
                      f.write(f"{qid} 0 {docno} 1\n")

      topic_docs = parse_run_file("runs/student.run")
      print(f"Found {len(topic_docs)} topics in run file")
      create_realistic_qrels(topic_docs, "out/qrels.demo.txt", 0.25)
      print(f"Created realistic qrels with relevance rate 0.25")
      print(f"Qrels written to: out/qrels.demo.txt")
      EOF
      echo "âœ… Evaluation data generated"

  - name: Evaluate Results
    run: |
      echo "ğŸ“ˆ Evaluating results..."
      python3 tools/evaluate.py out/qrels.demo.txt runs/student.run --out_csv out/standings.csv --out_md out/standings.md
      echo "âœ… Evaluation completed"

  - name: Display Results
    run: |
      echo "=== SEARCH ENGINE EVALUATION RESULTS ==="
      if [ -f "out/standings.md" ]; then
        echo "ğŸ“Š Performance Metrics:"
        cat out/standings.md
        echo ""
        echo "ğŸ“ˆ Detailed Results:"
        if [ -f "out/standings.csv" ]; then
          cat out/standings.csv
        fi
      else
        echo "âŒ No evaluation results found"
      fi

  - name: Publish Summary
    run: |
      echo "## ğŸ” CS7IS3 Assignment 2 - Search Engine Evaluation" >> $GITHUB_STEP_SUMMARY
      echo "" >> $GITHUB_STEP_SUMMARY
      echo "### ğŸ“Š Performance Metrics" >> $GITHUB_STEP_SUMMARY
      if [ -f "out/standings.md" ]; then
        cat out/standings.md >> $GITHUB_STEP_SUMMARY
      else
        echo "âŒ No evaluation results available" >> $GITHUB_STEP_SUMMARY
      fi
      echo "" >> $GITHUB_STEP_SUMMARY
      echo "### ğŸ“ˆ Key Metrics:" >> $GITHUB_STEP_SUMMARY
      if [ -f "out/standings.csv" ]; then
        best=$(tail -n +2 out/standings.csv | head -n1)
        IFS=',' read -r rank run MAP P20 nDCG <<< "$best"
        echo "- **MAP (Mean Average Precision)**: $MAP" >> $GITHUB_STEP_SUMMARY
        echo "- **P@20 (Precision at 20)**: $P20" >> $GITHUB_STEP_SUMMARY
        echo "- **nDCG@20 (Normalized DCG at 20)**: $nDCG" >> $GITHUB_STEP_SUMMARY
        echo "map=$MAP" >> $GITHUB_OUTPUT
        echo "p20=$P20" >> $GITHUB_OUTPUT
        echo "ndcg20=$nDCG" >> $GITHUB_OUTPUT
      fi
    id: summary

  - name: Upload Artifacts
    uses: actions/upload-artifact@v4
    with:
      name: search-engine-evaluation-results
      path: |
        out/standings.csv
        out/standings.md
        runs/student.run
        out/qrels.demo.txt
```
